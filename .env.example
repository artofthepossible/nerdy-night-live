# ============================================
# Deployment Mode
# ============================================
# Set to 'local' to use local cagent runtime
DEPLOYMENT_MODE=local

# ============================================
# Server Configuration
# ============================================
PORT=8080

# ============================================
# AI Model Configuration
# ============================================
# AI Model Provider Configuration
# Supported providers: openai, anthropic, google, dmr (Docker Model Runner), bedrock
AI_PROVIDER=openai

# Model Configuration
# For OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo, etc.
# For Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229, etc.
# For Google: gemini-2.0-flash-exp, gemini-1.5-pro, etc.
# For DMR (local models): gemini-qat, llama-3.1-8b, etc.
# For Bedrock: anthropic.claude-3-5-sonnet-20241022-v2:0
AI_MODEL=gpt-4o-mini

# Maximum tokens for model responses
AI_MAX_TOKENS=4000

# Temperature (0.0-1.0, higher = more creative)
AI_TEMPERATURE=0.8

# ============================================
# API Keys (only needed for cloud providers)
# ============================================
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
GOOGLE_API_KEY=your_google_key_here

# ============================================
# Docker Model Runner Configuration (for local models)
# ============================================
# When using AI_PROVIDER=dmr, ensure Docker Desktop is running
# and the model is available via Docker Model Runner
# DMR_MODEL_NAME should match the model name in Docker Desktop
DMR_MODEL_NAME=gemini-qat
DMR_ENDPOINT=http://localhost:8080/v1

